# numc

Here's what I did in project 4:
-For Task 1, I implemented a naive version of matrix.c, a simple C program designed to perform some basic matrix operations, such as add, sub, mul, pow, neg, and abs. I implemented this by analyzing matrix.h and matrix.c to understand the matrix structure, and then proceeded to write allocate and deallocate matrices, closely following the specifications given for error codes, usage of ref_cnt, usage of parent, and handling of slices. Whenever I was unsure of whether something I was doing was correct, I would consult the Piazza for assistance, and there were always others who had similar questions. After I completed this task, I moved onto the actual matrix operation functions. These naive functions were relatively straightforward, as get and set were simply inputting and extracting data (similar to and arguably easier than Project 1), and fill, add, sub, neg, and abs were simply for loops adding / subtracting / filling / swapping the sign of / taking the absolute value of each index of the matrix based on the values given. For mul, being matrix multiplication and not simple multiplication, the implementation was slightly harder, but still relatively simple. I used three nested for loops to multiply the rows and columns together and stored it in result. For pow, I made a simple while loop which repeated the mul function on a temporary matrix and repeating until I achieved the desired power. Ultimately, this was fairly straightforward compared to the latter parts of the project. For Task 2, this was by far the shortest part of this project, but was arguably more difficult than the Task 1 because of the module installation syntax which we had to learn via the docs for. After reading up on how to fill out Extension and setup, I was able to compplete this portion of the project fairly easily. Again, Piazza was extremely helpful for debugging and other documentation and setup links to ensure the module installation worked. For Task 3, I found this to be fairly straighforward: the vast majority of this section was similar to the previous section in which I spent the majority of my time reading about the various methods and syntax documents required for the Python-C interface, and then proceeding to use these methods appropriately. The function methods were fairly simple, as I simply checked for invalid types, invalid dimensions, or failed memory allocations, threw the relevant PyErrs, and proceeded to input the function parameters into the relevant function, returning the result at the end. For the last two items, Matrix_61c_as_number struct and the Matrix61c_methods array, I read the relevant Python docs, and followed various other examples as seen online. Similarly to Task 2, the Piazza was especially helpful here for syntax errors and example sources, and I was able to correctly implement this fairly quickly with the assistance of Piazza comments on the same or similar bugs and links to other helpful sources. For Task 4, this was undisputably the most difficult, as it combined everything from all recent lectures into a fairly comprehensive optimization task. At first, I simply added omp whenever possible throughout all simple functions (I initially ignored mul and pow). However, I quickly realized that omp significantly impacts performance on smaller matrices, such that the speedup would consistently be less than a tenth of the naive solution. As such, I then added conditions to only use omp if the matrices were above a certain size. This allowed me to maintain reasonable matrix operation times for small matrices (unfortunately no speedup for small matrices), and for larger matrices I was able to observe up to a x1.5 speedup (still quite far from x4 speedup, however). At this point I moved onto mul and pow. This was easily the most painful part. I decided to implement what I thought was the easiest first: repeated squaring for pow. This was fairly simple, as I simply handled the 0 and 1 cases, then proceeded to square the matrix and half the power each time. However, I kept running into memory issues likely due to some faulty pointer handling while swapping the data around temp variables to use. At this moment, I realized I could actually use recursion to make my pow function look much cleaner while still using repeated squaring by passing mat^2 and pow / 2 into the same pow function. After reformatting my pow function to accomodate recursion, I was able to see my speedup increase to roughly x3 (still extremely far from x900, although that was likely almost entirely due to my mul function). My initial nested for loop implementation of mul indexed using result->rows, then mat1->cols, then result->cols. I decided to restructure this by swapping the inner two loops such that the ordering of the loops became result->rows, result->cols, and mat1->cols, using a sum variable to hold the temporary sum and then adding it into result. This allowed me to use omp on the outermost loop, allowing for some speedup. However, the speedup was still only in the single digits, not even passing x10 speedup. At this point, I decided to comb through lecture slides / lab09 and add unrolling and SIMD instructions to assist in speedup; however, this did not improve my speedup significantly, and it was still quite far away from the x90 speedup benchmark. Moreover, this tanked my pow speedup - while my mul speedup was finally in the dozens, my pow speedup tanked tremendously, dropping to the single digits or even less. From this point, I made a multitude of changes to mul, effectively rewriting the entire function roughly three times, adding and changing various aspects. After some time, I was finally able to make pow to above x1 speedup; however, it was still far from good. At this point, I looked to Piazza again for suggestions, and after reading through a multitude of comments suggesting transpose, I decided to implement it. This drastically increased my performance, instantly putting mul into multiple dozens higher of speedup. However, I realized that in my various revisions of mul for fixing pow, I accidentally made it so that it could only handle square matrices. After reading on Piazza and noticing others created separate mul functions for pow and mul itself, I decided to attempt something similar, and created two separate mul functions, one for pow and one for mul. After reading more on Piazza, I also realized that fill_matrix is not optimal, and that memcpy, memset, and calloc were all better alternatives for copying, filling, and clearing (setting all values to 0) matrices. After using memcpy, memset, and calloc, I instantly noticed my simple speedup was suddenly x4 or x5, and my mul was up to roughly x80 (for medium sized matrices). Ultimately, Piazza, lab09, and lab10 were really the key contributors to my project here today. Something to note is that across all the changes and modifications I did to test speedup, by far the least useful was the SIMD instructions. I felt that the SIMD instructions barely helped if at all in speeding up functions, with the vast majority of speedup occuring in omp, transpose, calloc, and repeated squaring.